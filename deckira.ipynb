{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBRWsPnzznEN",
        "outputId": "5422b9d3-b588-48d5-8b10-80b546f7fa14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the path to the CSV file: /content/plays_football.csv\n",
            "\n",
            "Available columns: Outlook, Temperature, Humidity, Windy, Class\n",
            "Enter the name of the target column: Class\n",
            "\n",
            "Step 1: Calculating Initial Entropy (Info(D))\n",
            "Initial Entropy (Info(D)): 0.9403\n",
            "\n",
            "Step 2: Calculating Information Gain for Each Attribute\n",
            "Information Gain for Outlook: 0.2467\n",
            "Information Gain for Temperature: 0.0292\n",
            "Information Gain for Humidity: 0.1518\n",
            "Information Gain for Windy: 0.0161\n",
            "\n",
            "Step 3: Building the Decision Tree\n",
            "Information Gain for Outlook: 0.2467\n",
            "Information Gain for Temperature: 0.0292\n",
            "Information Gain for Humidity: 0.1518\n",
            "Information Gain for Windy: 0.0161\n",
            "\n",
            "Best attribute to split on: Outlook\n",
            "\n",
            "Information Gain for Temperature: 0.0200\n",
            "Information Gain for Humidity: 0.0200\n",
            "Information Gain for Windy: 0.9710\n",
            "\n",
            "Best attribute to split on: Windy\n",
            "\n",
            "Information Gain for Temperature: 0.5710\n",
            "Information Gain for Humidity: 0.9710\n",
            "Information Gain for Windy: 0.4200\n",
            "\n",
            "Best attribute to split on: Humidity\n",
            "\n",
            "\n",
            "Step 4: Decision Tree Rules\n",
            "IF Outlook = Rain AND Windy = Weak THEN Class = Yes\n",
            "IF Outlook = Rain AND Windy = Strong THEN Class = No\n",
            "IF Outlook = Sunny AND Humidity = Normal THEN Class = Yes\n",
            "IF Outlook = Sunny AND Humidity = High THEN Class = No\n",
            "IF Outlook = Overcast THEN Class = Yes\n",
            "\n",
            "Step 5: Text-Based Tree Visualization\n",
            "Outlook\n",
            "├── Rain\n",
            "│   Windy\n",
            "│   ├── Weak\n",
            "│   │   └── Class: Yes\n",
            "│   ├── Strong\n",
            "│   │   └── Class: No\n",
            "├── Sunny\n",
            "│   Humidity\n",
            "│   ├── Normal\n",
            "│   │   └── Class: Yes\n",
            "│   ├── High\n",
            "│   │   └── Class: No\n",
            "├── Overcast\n",
            "│   └── Class: Yes\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_entropy(data, target_column):\n",
        "    class_counts = Counter([row[target_column] for row in data])\n",
        "    total_rows = len(data)\n",
        "    entropy = 0.0\n",
        "    for count in class_counts.values():\n",
        "        probability = count / total_rows\n",
        "        entropy -= probability * math.log2(probability)\n",
        "    return entropy\n",
        "\n",
        "def calculate_information_gain(data, attribute, target_column):\n",
        "    total_entropy = calculate_entropy(data, target_column)\n",
        "    attribute_values = set([row[attribute] for row in data])\n",
        "    weighted_entropy = 0.0\n",
        "    total_rows = len(data)\n",
        "    for value in attribute_values:\n",
        "        subset = [row for row in data if row[attribute] == value]\n",
        "        subset_size = len(subset)\n",
        "        subset_entropy = calculate_entropy(subset, target_column)\n",
        "        weighted_entropy += (subset_size / total_rows) * subset_entropy\n",
        "    information_gain = total_entropy - weighted_entropy\n",
        "    return information_gain\n",
        "\n",
        "def find_best_attribute(data, attributes, target_column):\n",
        "    best_attribute = None\n",
        "    max_gain = -1\n",
        "    for attribute in attributes:\n",
        "        gain = calculate_information_gain(data, attribute, target_column)\n",
        "        print(f\"Information Gain for {attribute}: {gain:.4f}\")\n",
        "        if gain > max_gain:\n",
        "            max_gain = gain\n",
        "            best_attribute = attribute\n",
        "    return best_attribute\n",
        "\n",
        "def build_decision_tree(data, attributes, target_column, tree=None):\n",
        "    if tree is None:\n",
        "        tree = {}\n",
        "    classes = [row[target_column] for row in data]\n",
        "    if len(set(classes)) == 1:\n",
        "        return classes[0]\n",
        "    if not attributes:\n",
        "        majority_class = Counter(classes).most_common(1)[0][0]\n",
        "        return majority_class\n",
        "    best_attribute = find_best_attribute(data, attributes, target_column)\n",
        "    print(f\"\\nBest attribute to split on: {best_attribute}\\n\")\n",
        "    tree = {best_attribute: {}}\n",
        "    remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n",
        "    attribute_values = set([row[best_attribute] for row in data])\n",
        "    for value in attribute_values:\n",
        "        subset = [row for row in data if row[best_attribute] == value]\n",
        "        if not subset:\n",
        "            majority_class = Counter(classes).most_common(1)[0][0]\n",
        "            tree[best_attribute][value] = majority_class\n",
        "        else:\n",
        "            subtree = build_decision_tree(subset, remaining_attributes, target_column)\n",
        "            tree[best_attribute][value] = subtree\n",
        "    return tree\n",
        "\n",
        "def classify(tree, instance):\n",
        "    if isinstance(tree, str):\n",
        "        return tree\n",
        "    attribute = next(iter(tree))\n",
        "    attribute_value = instance.get(attribute)\n",
        "    if attribute_value not in tree[attribute]:\n",
        "        return None\n",
        "    subtree = tree[attribute][attribute_value]\n",
        "    return classify(subtree, instance)\n",
        "\n",
        "def print_rules(tree, rule=\"\"):\n",
        "    if isinstance(tree, str):\n",
        "        print(f\"IF {rule} THEN Class = {tree}\")\n",
        "        return\n",
        "    attribute = next(iter(tree))\n",
        "    for value, subtree in tree[attribute].items():\n",
        "        new_rule = f\"{rule} AND {attribute} = {value}\" if rule else f\"{attribute} = {value}\"\n",
        "        print_rules(subtree, new_rule)\n",
        "\n",
        "def print_tree(tree, indent=\"\"):\n",
        "    if isinstance(tree, str):\n",
        "        print(indent + \"└── Class:\", tree)\n",
        "        return\n",
        "    attribute = next(iter(tree))\n",
        "    print(indent + attribute)\n",
        "    for value, subtree in tree[attribute].items():\n",
        "        print(indent + \"├──\", value)\n",
        "        print_tree(subtree, indent + \"│   \")\n",
        "\n",
        "def load_csv_data(file_path, target_column):\n",
        "    df = pd.read_csv(file_path)\n",
        "    if target_column not in df.columns:\n",
        "        raise ValueError(f\"Target column '{target_column}' not found in dataset.\")\n",
        "    attributes = [col for col in df.columns if col != target_column]\n",
        "    data = df.to_dict(orient='records')\n",
        "    return data, attributes, target_column\n",
        "\n",
        "def decision_tree_main():\n",
        "    file_path = input(\"Enter the path to the CSV file: \").strip()\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"\\nAvailable columns:\", ', '.join(df.columns))\n",
        "    target_column = input(\"Enter the name of the target column: \").strip()\n",
        "    data, attributes, target_column = load_csv_data(file_path, target_column)\n",
        "\n",
        "    print(\"\\nStep 1: Calculating Initial Entropy (Info(D))\")\n",
        "    initial_entropy = calculate_entropy(data, target_column)\n",
        "    print(f\"Initial Entropy (Info(D)): {initial_entropy:.4f}\\n\")\n",
        "\n",
        "    print(\"Step 2: Calculating Information Gain for Each Attribute\")\n",
        "    for attribute in attributes:\n",
        "        gain = calculate_information_gain(data, attribute, target_column)\n",
        "        print(f\"Information Gain for {attribute}: {gain:.4f}\")\n",
        "\n",
        "    print(\"\\nStep 3: Building the Decision Tree\")\n",
        "    decision_tree = build_decision_tree(data, attributes, target_column)\n",
        "\n",
        "    print(\"\\nStep 4: Decision Tree Rules\")\n",
        "    print_rules(decision_tree)\n",
        "\n",
        "    print(\"\\nStep 5: Text-Based Tree Visualization\")\n",
        "    print_tree(decision_tree)\n",
        "\n",
        "# Run the decision tree main function\n",
        "decision_tree_main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x6csDKez0E5H"
      }
    }
  ]
}